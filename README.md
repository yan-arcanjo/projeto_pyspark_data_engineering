![Spark](https://img.shields.io/badge/spark-3.4.2-orange)
![Delta Lake](https://img.shields.io/badge/delta--lake-2.4.0-blue)
![Airflow](https://img.shields.io/badge/airflow-2.7.3-green)
![Status](https://img.shields.io/badge/status-em%20desenvolvimento-yellow)

# üîó Projeto de Engenharia de Dados ‚Äì CoinGecko Pipeline

Este projeto simula um pipeline completo de engenharia de dados, utilizando extra√ß√µes da [API CoinGecko](https://www.coingecko.com/) e tecnologias como **Apache Airflow**, **PySpark** e **Delta Lake** para transformar os dados em camadas `stage`, `bronze` e `silver`, com foco em aplica√ß√µes anal√≠ticas e estruturadas para consumo via Power BI.

---

## üìä Estrutura do Projeto

```bash
üìÜ src/
 ‚îú‚îÄ‚îÄ jobs/
 ‚îÇ   ‚îú‚îÄ‚îÄ 1-stage/        # Scripts para extrair e salvar arquivos JSON (dados brutos da API)
 ‚îÇ   ‚îú‚îÄ‚îÄ 2-bronze/       # Scripts PySpark para criar tabelas Delta com hist√≥rico completo
 ‚îÇ   ‚îî‚îÄ‚îÄ 3-silver/       # Scripts PySpark para gerar tabelas Delta com o dado mais recente
 ‚îî‚îÄ‚îÄ logs/               # Tabela de logs de execu√ß√£o
```

---

## üß™ Tecnologias Utilizadas

* **Airflow Standalone** (2.7.3) ‚Äì agendamento e orquestra√ß√£o de pipelines
* **PySpark (3.4.2)** com Delta Lake (2.4.0)
* **API CoinGecko** ‚Äì extra√ß√£o de dados de criptomoedas em tempo real
* **Delta Lake** ‚Äì controle de vers√µes e transa√ß√µes ACID nas camadas Bronze e Silver

---

## üîÑ Camadas do Pipeline

| Camada   | Descri√ß√£o                                                                                                                                                                             |
| -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `stage`  | Cont√©m os arquivos JSON extra√≠dos diretamente da API. S√£o salvos com timestamp e armazenam os dados brutos.                                                                           |
| `bronze` | Cont√©m tabelas Delta com o **hist√≥rico completo** de cada chamada da API. H√° controle por **chaves prim√°rias** (ex: `id`) e **hash de todas as colunas** para identificar altera√ß√µes. |
| `silver` | Cont√©m a **√∫ltima vers√£o** de cada moeda, mantendo apenas os registros mais recentes e limpos para consumo em BI.                                                                     |

---

## ‚öôÔ∏è Orquestra√ß√£o com Airflow

O pipeline √© composto por 3 tasks principais no DAG:

1. **`extract_api_data`** ‚Üí Extrai os dados da CoinGecko e salva como JSON na pasta `stage/`
2. **`run_bronze_script`** ‚Üí Executa script PySpark via `spark-submit` para gerar/atualizar a camada Bronze
3. **`run_silver_script`** ‚Üí Executa script PySpark para atualizar a camada Silver com os dados mais recentes

### üì∑ Visualiza√ß√£o do DAG

![DAG Airflow](imgs/dag_airflow.png)

### üîß Exemplo da Task `run_bronze_script`:

```python
run_bronze_script = BashOperator(
    task_id='run_bronze_script',
    bash_command='spark-submit --packages io.delta:delta-core_2.12:2.4.0 \
        --master local[*] \
        --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
        --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
        --conf "spark.driver.extraJavaOptions=-Dlog4j.configurationFile=src/jobs/log4j2.properties" \
        /mnt/c/Users/arcan/Documents/airflow/src/jobs/2-bronze/coingecko/coingecko.py coingecko id',
)
```

> Os argumentos s√£o passados via `sys.argv`, permitindo scripts gen√©ricos e reutiliz√°veis.

---

## üìù Logs de Execu√ß√£o

O pipeline grava uma tabela Delta de logs, contendo:

* task\_id
* status (SUCESSO, ERRO)
* timestamp
* mensagem de erro (se houver)

---

## üöÄ Instru√ß√µes para Executar o Projeto

### ‚≠êÔ∏è Subindo o Airflow Standalone (ambiente de estudo)

```bash
# 1. Criar diret√≥rio e ambiente virtual
python -m venv venv && source venv/bin/activate

# 2. Instalar Airflow
pip install "apache-airflow==2.7.3" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.7.3/constraints-3.9.txt"

# 3. Definir vari√°veis de ambiente
export AIRFLOW_HOME=~/opt/airflow
export AIRFLOW__CORE__LOAD_EXAMPLES=False

# 4. Inicializar o Airflow standalone
airflow standalone
```

---

### ‚ö° Subindo o Apache Spark com Delta (ambiente local)

```bash
# 1. Instalar Java
sudo apt-get install openjdk-8-jdk-headless -qq

# 2. Instalar bibliotecas PySpark e Delta
pip install pyspark==3.4.2 delta-spark==2.4.0
```

### üßü SparkSession com Delta

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .master("local[*]") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:2.4.0") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()
```

---

### ‚ñ∂Ô∏è Rodando scripts com `spark-submit`

```bash
spark-submit --packages io.delta:delta-core_2.12:2.4.0 \
--conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
--conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
--conf "spark.driver.extraJavaOptions=-Dlog4j.configurationFile=src/jobs/log4j2.properties" \
./src/jobs/2-bronze/coingecko.py coingecko id
```

---

## üìä Consumo dos dados

Os dados da camada `silver/` e `gold/` podem ser consumidos por ferramentas de BI como o **Power BI**, apontando diretamente para os arquivos `.parquet` gerados, localmente ou em um diret√≥rio de rede.

---

## üìå Roadmap futuro

* [ ] Criar camada `gold/` com vis√µes anal√≠ticas (top moedas, varia√ß√µes, market cap)
* [ ] Publicar painel em Power BI ou Streamlit
* [ ] Vers√£o com Delta Lake + Spark em cluster (Dataproc ou EMR)

---

## üßë‚Äçüíª Autor

**Yan Arcanjo**
Desenvolvedor de pipelines anal√≠ticos com foco em dados organizados e rastre√°veis usando Spark, Airflow, e arquitetura medallion.

---

> *Este projeto foi desenvolvido para fins de estudo e demonstra√ß√£o pr√°tica de um pipeline de dados moderno, com ingest√£o, versionamento e transforma√ß√£o robusta de dados.*
